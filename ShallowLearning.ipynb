{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data handling & basic libraries\n",
    "import numpy as np\n",
    "from numpy import ma\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "#Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Data Augmentation\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "\n",
    "#Gridsearch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "#Models\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#Visualization\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "#Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data & subset by labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi loader data, dataset subsettes ved længden af labels og vi arrangerer farvebånd til RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fields\n",
    "# ccFieldsAll = np.load(\"C:/Users/mkoli/Syddansk Universitet/Morten Thyrring Stouenberg - Speciale2023/Data/AllNoAlt/ccLastImageNoCloudIndex0_6234.npy\", allow_pickle=True)\n",
    "ccFieldsAll = np.load(\"C:/Users/morte/OneDrive - Syddansk Universitet/Speciale2023/Data/AllNoAlt/ccLastImageNoCloudIndex0_6234.npy\", allow_pickle=True)\n",
    "\n",
    "# Labels\n",
    "# labels = np.load(\"C:/Users/mkoli/Syddansk Universitet/Morten Thyrring Stouenberg - Speciale2023/Data/AllNoAlt/reLabelsAll.npy\", allow_pickle=True)\n",
    "labels = np.load(\"C:/Users/morte/OneDrive - Syddansk Universitet/Speciale2023/Data/AllNoAlt/reLabelsAll.npy\", allow_pickle=True)\n",
    "\n",
    "# Cloud probability\n",
    "cloud_probability = ccFieldsAll[:len(labels),:,:,7]\n",
    "\n",
    "# Subset data to true labels\n",
    "ccFieldsAll = ccFieldsAll[:len(labels),:,:,[2,1,0]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cloud masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We Set cloud tolerance, between 0-255\n",
    "cloud_tolerance = 40\n",
    "    \n",
    "# We loop in the fields by the length of the dataset\n",
    "for i in range(len(ccFieldsAll)):\n",
    "    # we subset the image we are at in the loop\n",
    "    image = ccFieldsAll[i,:,:,:]    \n",
    "    # We create and apply a mask based on cloud probability over a certain value\n",
    "    # We use np.logical because np.where did not work for me at the moment\n",
    "    mask = np.logical_and(cloud_probability[i, :, :,] >= cloud_tolerance, cloud_probability[i, :, :] <= 256)\n",
    "    # We set all masked pixels to zero, to black out cloud covered parts\n",
    "    # I do not know why but it didn't work when i did not make a copy. It is just one of those times. \n",
    "    masked = image.copy()\n",
    "    masked[mask] = 0\n",
    "    # We are replacing the current image with the one with applied cloud mask, if no clouds then nothing is removed.\n",
    "    ccFieldsAll[i] = masked"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Delete empty images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to delete the all black images, this is done by only appending images with visible RGB bands not all black.\n",
    "# Initiate list for images that did not get blacked out as well as a list for labels\n",
    "ccFieldsNoCloud = []\n",
    "LabelsNoCloud = []\n",
    "\n",
    "# We loop in all the images\n",
    "for i in range(len(ccFieldsAll)):\n",
    "    # if the image is not all black\n",
    "    if not np.all(ccFieldsAll[i] == 0):\n",
    "        #we append the image and label\n",
    "        ccFieldsNoCloud.append(ccFieldsAll[i])\n",
    "        LabelsNoCloud.append(labels[i])\n",
    "\n",
    "\n",
    "# We set the lists as arrays\n",
    "ccFieldsNoCloud = np.array(ccFieldsNoCloud)\n",
    "LabelsNoCloud   = np.array(LabelsNoCloud)\n",
    "\n",
    "#And print the shapes to control the output and see how many fields are left.\n",
    "print(ccFieldsNoCloud.shape)\n",
    "print(LabelsNoCloud)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We select 64x64 as image resolution since this is the best performing models measured on accuracy for both Random Forest, SVC and XGboost\n",
    "#we use tensorflows image.resize function for resizing. 64x64 is the chosen size based on performance of models.\n",
    "\n",
    "size = 64\n",
    "\n",
    "ccFieldsResized = []\n",
    "for i in ccFieldsAll:\n",
    "    resized_image = tf.image.resize(i, [size, size])\n",
    "    ccFieldsResized.append(resized_image)\n",
    "ccFieldsResized = np.stack(ccFieldsResized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,30))\n",
    "for i in range(50):\n",
    "    plt.subplot(10,5,i+1)\n",
    "    plt.imshow(3.5* ccFieldsResized[i] / 10000)\n",
    "    plt.title(f'\\n idx: {i}, label: {labels[i]}')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flatten data into 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To flatten data into 2dimensional data we first obtain the number of observations\n",
    "num_samples = ccFieldsResized.shape[0]\n",
    "# We obtain the image size as the height, width and channels\n",
    "image_size = ccFieldsResized.shape[1] * ccFieldsResized.shape[2] * ccFieldsResized.shape[3]\n",
    "# we use numpys reshape function and utilize the information obtained in the previous steps for flattening\n",
    "ccFieldsResized_2d = np.reshape(ccFieldsResized, (num_samples, image_size))\n",
    "\n",
    "ccFieldsResized_2d.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomState = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(ccFieldsResized_2d, \n",
    "                                                    labels, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=randomState)\n",
    "\n",
    "# Removed because we uses cross validation grid search\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, \n",
    "#                                                   y_train, \n",
    "#                                                   test_size=0.25, \n",
    "#                                                   random_state=1) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "print(X_train.shape, y_train.shape) \n",
    "print(X_test.shape, y_test.shape)\n",
    "# print(X_val.shape, y_val.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labels distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of observations : {len(LabelsNoCloud)}')\n",
    "print(f'appearances of 0 in labels : {np.count_nonzero(LabelsNoCloud == 0)}({round(100*np.count_nonzero(LabelsNoCloud == 0)/len(LabelsNoCloud),2)}% of total)')\n",
    "print(f'appearances of 1 in labels : {np.count_nonzero(LabelsNoCloud == 1)}({round(100*np.count_nonzero(LabelsNoCloud == 1)/len(LabelsNoCloud),2)}% of total)')\n",
    "print(f'appearances of 2 in labels : {np.count_nonzero(LabelsNoCloud == 2)}({round(100*np.count_nonzero(LabelsNoCloud == 2)/len(LabelsNoCloud),2)}% of total)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Base model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A base model is initiated\n",
    "baseRF = RandomForestClassifier(random_state=randomState,\n",
    "                                n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The base model is fit on the training data \n",
    "baseRF.fit(X_train, y_train)\n",
    "# Base model presictions are made\n",
    "y_pred = baseRF.predict(X_test)\n",
    "# A classification report is printed based on the predictions of the model\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Gridsearch & optimal model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A base model is initiated for a gridsearch approach\n",
    "clfRF = RandomForestClassifier(random_state=randomState,\n",
    "                               n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining parameter range for gridsearch\n",
    "n_estimators = [100, 200, 300, 500, 800, 1000, 1500]                  # number of trees in the random forest (100)\n",
    "max_features = ['log2', 'sqrt', 0.1, None]           # number of features in consideration at every split (sqrt)\n",
    "max_depth = [5, 30, 50, 70, None]    # maximum number of levels allowed in each decision tree (None)\n",
    "min_samples_split = [2, 5, 10, 15]              # minimum sample number to split a node (2)\n",
    "min_samples_leaf = [1, 2, 5, 10]                    # minimum sample number that can be stored in a leaf node (1)\n",
    "\n",
    "# defining parameter range for gridsearch\n",
    "n_estimators = [200, 300, 400, 800]                  # number of trees in the random forest (100)\n",
    "max_features = ['sqrt', 0.1]           # number of features in consideration at every split (sqrt)\n",
    "max_depth = [30, 50, 70, None]                   # maximum number of levels allowed in each decision tree (None)\n",
    "min_samples_split = [2]              # minimum sample number to split a node (2)\n",
    "min_samples_leaf = [2, 5, 10]                    # minimum sample number that can be stored in a leaf node (1)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_features': max_features,\n",
    "    'max_depth': max_depth,\n",
    "    'min_samples_split': min_samples_split,\n",
    "    'min_samples_leaf': min_samples_leaf\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the gridsearch, the base estimator clfRF is utilized with the parameter grid specified above. \n",
    "gridSearchRF = GridSearchCV(estimator = clfRF, \n",
    "                            param_grid = param_grid,\n",
    "                            refit = True,\n",
    "                            verbose = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The gridsearch of the estimator is fit to go through the different combinations of hyperparameters\n",
    "gridSearchRF.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print how our model looks after hyper-parameter tuning\n",
    "print('\\n Best estimator:')\n",
    "print(gridSearchRF.best_estimator_)\n",
    "\n",
    "# print mean cross-validated score of the best model\n",
    "print('/n Best score:')\n",
    "print(gridSearchRF.best_score_)\n",
    "\n",
    "# print best parameter after tuning\n",
    "print('\\n Best parameters:')\n",
    "print(gridSearchRF.best_params_)\n",
    "\n",
    "# print top ten results of gridsearch\n",
    "print('\\n Top ten results:')\n",
    "resultsRF = pd.DataFrame(gridSearchRF.cv_results_)\n",
    "resultsRF.sort_values(by='rank_test_score', inplace=True)\n",
    "resultsRF = resultsRF.head(30)\n",
    "display(resultsRF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "fPath = 'C:/Users/morte/OneDrive - Syddansk Universitet/Speciale2023/Models/RF/gridSearchRF_Run2.sav'\n",
    "\n",
    "joblib.dump(gridSearchRF, open(fPath, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "fPath = 'C:/Users/morte/OneDrive - Syddansk Universitet/Speciale2023/Models/RF/gridSearchRF_Run2.sav'\n",
    "clfRFLoaded = joblib.load(open(fPath, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print how our model looks after hyper-parameter tuning\n",
    "print('\\n Best estimator:')\n",
    "print(clfRFLoaded.best_estimator_)\n",
    "\n",
    "# print mean cross-validated score of the best model\n",
    "print('/n Best score:')\n",
    "print(clfRFLoaded.best_score_)\n",
    "\n",
    "# print best parameter after tuning\n",
    "print('\\n Best parameters:')\n",
    "print(clfRFLoaded.best_params_)\n",
    "\n",
    "# print top ten results of gridsearch\n",
    "print('\\n Top ten results:')\n",
    "resultsRF = pd.DataFrame(clfRFLoaded.cv_results_)\n",
    "resultsRF.sort_values(by='rank_test_score', inplace=True)\n",
    "resultsRF.to_csv('C:/Users/morte/OneDrive - Syddansk Universitet/Speciale2023/Models/RF/gridSearchRF_Run2.csv')  \n",
    "resultsRF = resultsRF.head(30)\n",
    "display(resultsRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on the test set\n",
    "y_pred = clfRFLoaded.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Define the class labels\n",
    "class_names = ['Class 0', 'Class 1', 'Class 2']\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, cmap=plt.cm.Reds)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.xticks(np.arange(len(class_names)), class_names, rotation=45)\n",
    "plt.yticks(np.arange(len(class_names)), class_names)\n",
    "plt.colorbar()\n",
    "\n",
    "# Add labels to the plot\n",
    "for i in range(len(class_names)):\n",
    "    for j in range(len(class_names)):\n",
    "        plt.text(j, i, cm[i, j], ha='center', va='center', color='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Base model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseXGB = XGBClassifier(objective='multi:softmax',\n",
    "                        num_class = 3,\n",
    "                        n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseXGB.fit(X_train, y_train)\n",
    "y_pred = baseXGB.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Gridsearch & optimal model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining parameter range for gridsearch\n",
    "# Values from text\n",
    "learning_rate = [0.0001, 0.001, 0.01, 0.1] #2   # Step size shrinkage used in update to prevents overfitting (0.1). After each boosting step, we can directly get the weights of new features, and learning_rate shrinks the feature weights to make the boosting process more conservative.\n",
    "learning_rate = [0.001, 0.01]\n",
    "n_estimators = [100, 300, 500, 800, 1000, 1500] #3          # number of trees in the random forest (100)\n",
    "n_estimators = [800, 1000] \n",
    "min_child_weight = [1, 5, 10]             # Minimum sum of instance weight (hessian) needed in a child (1). If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In linear regression task, this simply corresponds to minimum number of instances needed to be in each node. The larger min_child_weight is, the more conservative the algorithm will be.\n",
    "min_child_weight = [1, 5]\n",
    "subsample = [0.6, 0.8, 1.0]               # Subsample ratio of the training instances (1). Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. Subsampling will occur once in every boosting iteration.\n",
    "subsample = [0.6, 0.8, 1.0]\n",
    "colsample_bytree = [0.6, 0.8, 1.0]        # colsample_bytree is the subsample ratio of columns when constructing each tree. Subsampling occurs once for every tree constructed.\n",
    "colsample_bytree = [0.01, 0.1]\n",
    "max_depth = [3, 4, 5]                     # Maximum depth of a tree (6). Increasing this value will make the model more complex and more likely to overfit. 0 indicates no limit on depth. Beware that XGBoost aggressively consumes memory when training a deep tree. exact tree method requires non-zero value.\n",
    "max_depth = [6, 10]\n",
    "\n",
    "params_grid = {\n",
    "   'learning_rate' : learning_rate,\n",
    "   'n_estimators': n_estimators,\n",
    "   'min_child_weight': min_child_weight,\n",
    "   'gamma': gamma,\n",
    "   'subsample': subsample,\n",
    "   'colsample_bytree': colsample_bytree,\n",
    "   'max_depth': max_depth\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classifier\n",
    "clfXGB = XGBClassifier(\n",
    "    objective='multi:softmax',\n",
    "    num_class = 3,\n",
    "    n_jobs = -1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Grid Search\n",
    "gridSearchXGB = GridSearchCV(\n",
    "    estimator = clfXGB, \n",
    "    param_grid = params_grid, \n",
    "    refit = True, \n",
    "    verbose = 3\n",
    "    )\n",
    "\n",
    "gridSearchXGB.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print how our model looks after hyper-parameter tuning\n",
    "print('\\n Best estimator:')\n",
    "print(gridSearchXGB.best_estimator_)\n",
    "\n",
    "# print mean cross-validated score of the best model\n",
    "print('/n Best score:')\n",
    "print(gridSearchXGB.best_score_)\n",
    "\n",
    "# print best parameter after tuning\n",
    "print('\\n Best parameters:')\n",
    "print(gridSearchXGB.best_params_)\n",
    "\n",
    "# print top ten results of gridsearch\n",
    "print('\\n Top ten results:')\n",
    "resultsXGB = pd.DataFrame(gridSearchXGB.cv_results_)\n",
    "resultsXGB.sort_values(by='rank_test_score', inplace=True)\n",
    "resultsXGB = resultsXGB.head(30)\n",
    "display(resultsXGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "fPath = 'C:/Users/morte/OneDrive - Syddansk Universitet/Speciale2023/Models/XGB/gridSearchXGB_Run3.sav'\n",
    "joblib.dump(gridSearchXGB, open(fPath, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "fPath = 'C:/Users/morte/OneDrive - Syddansk Universitet/Speciale2023/Models/XGB/gridSearchXGB_Run3.sav'\n",
    "clfXGBLoaded = joblib.load(open(fPath, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print how our model looks after hyper-parameter tuning\n",
    "print('\\n Best estimator:')\n",
    "print(clfXGBLoaded.best_estimator_)\n",
    "\n",
    "# print mean cross-validated score of the best model\n",
    "print('/n Best score:')\n",
    "print(clfXGBLoaded.best_score_)\n",
    "\n",
    "# print best parameter after tuning\n",
    "print('\\n Best parameters:')\n",
    "print(clfXGBLoaded.best_params_)\n",
    "\n",
    "# print top ten results of gridsearch\n",
    "print('\\n Top ten results:')\n",
    "resultsXGB = pd.DataFrame(clfXGBLoaded.cv_results_)\n",
    "resultsXGB.sort_values(by='rank_test_score', inplace=True)\n",
    "resultsXGB.to_csv('C:/Users/morte/OneDrive - Syddansk Universitet/Speciale2023/Models/XGB/gridSearchXGB_Run3.csv')  \n",
    "resultsXGB = resultsXGB.head(30)\n",
    "display(resultsXGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on the test set\n",
    "y_pred = clfXGBLoaded.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Define the class labels\n",
    "class_names = ['Class 0', 'Class 1', 'Class 2']\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, cmap=plt.cm.Reds)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.xticks(np.arange(len(class_names)), class_names, rotation=45)\n",
    "plt.yticks(np.arange(len(class_names)), class_names)\n",
    "plt.colorbar()\n",
    "\n",
    "# Add labels to the plot\n",
    "for i in range(len(class_names)):\n",
    "    for j in range(len(class_names)):\n",
    "        plt.text(j, i, cm[i, j], ha='center', va='center', color='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine resolution size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For determining the resolution size of the images, we run the different image sizes in an out of bag Random Forest classifier and compare the results. \n",
    "\n",
    "The steps utilized are the same as described in the models above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolutionList = [8, 16, 32, 64, 128, 256]\n",
    "\n",
    "clfRF = RandomForestClassifier(random_state=randomState,\n",
    "                               n_jobs=-1)\n",
    "                               \n",
    "clfXGB = XGBClassifier(objective='multi:softmax',\n",
    "                        num_class = 3,\n",
    "                        n_jobs = -1)\n",
    "\n",
    "for item in resolutionList:\n",
    "    size = item\n",
    "\n",
    "    ccFieldsResized = []\n",
    "    for i in ccFieldsAll:\n",
    "        resized_image = tf.image.resize(i, [size, size])\n",
    "        ccFieldsResized.append(resized_image)\n",
    "    ccFieldsResized = np.stack(ccFieldsResized)\n",
    "\n",
    "    num_samples = ccFieldsResized.shape[0]\n",
    "    image_size = ccFieldsResized.shape[1] * ccFieldsResized.shape[2] * ccFieldsResized.shape[3]\n",
    "    ccFieldsResized_2d = np.reshape(ccFieldsResized, (num_samples, image_size))\n",
    "\n",
    "    ccFieldsResized_2d.shape\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(ccFieldsResized_2d, \n",
    "                                                    labels, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=randomState)\n",
    "\n",
    "    print(X_train.shape, y_train.shape) \n",
    "    print(X_test.shape, y_test.shape)\n",
    "\n",
    "    clfRF.fit(X_train, y_train)\n",
    "    y_pred = clfRF.predict(X_test)\n",
    "    print(f'Classification report Random Forest for resolution: {item}')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    clfXGB.fit(X_train, y_train)\n",
    "    y_pred = clfXGB.predict(X_test)\n",
    "    print(f'Classification report XGBoost for resolution: {item}')\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section should be run before the classifier sections to train the models on augmented data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining augmentation method \n",
    " \n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range      = 360,        # Vi roterer billedet random\n",
    "    zoom_range          = 0.2,           # Zoomer random\n",
    "    brightness_range    = (0.8,1.2),    # Brightness range is random between 0.8 and 1.2 , 1 is the original brightness.  \n",
    "    horizontal_flip     =True,\n",
    "    vertical_flip       =False,\n",
    "    fill_mode='nearest')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_augmentation = []\n",
    "labels_for_augmentation = []\n",
    "\n",
    "#Loop data and append fields to each class for single class data augmentation \n",
    "for i in range(len(y_train)):\n",
    "  if y_train[i] == 0:\n",
    "    data_for_augmentation.append(X_train[i])\n",
    "    labels_for_augmentation.append(y_train[i])\n",
    "  if y_train[i] == 1:\n",
    "    data_for_augmentation.append(X_train[i])\n",
    "    labels_for_augmentation.append(y_train[i])\n",
    "\n",
    "# save as numpy arrays\n",
    "data_for_augmentation    = np.asarray(data_for_augmentation)\n",
    "labels_for_augmentation    = np.asarray(labels_for_augmentation)\n",
    "\n",
    "# Check  array shape\n",
    "print(f'data for augmentation : {data_for_augmentation.shape} labels for augmentation : {labels_for_augmentation.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Her sætter vi antallet af augmenterede billeder per billede\n",
    "num_samples_per_image = 7\n",
    "# num_samples_per_image = 9\n",
    "# num_samples_per_image = 11\n",
    "\n",
    "# Reshaping data\n",
    "aug_dataset            = data_for_augmentation.reshape(data_for_augmentation.shape[0], \n",
    "                                                      data_for_augmentation.shape[1], \n",
    "                                                      data_for_augmentation.shape[2], \n",
    "                                                      3)\n",
    "\n",
    "# Initiate list of labels for augmented data\n",
    "labels_aug = []\n",
    "\n",
    "#tomt array der passer til det vi gerne vil stoppe i\n",
    "augmented_images = []\n",
    "\n",
    "#index counter som sætter index i nyt array for append\n",
    "index_counter = 0\n",
    "\n",
    "for i in range(len(aug_dataset)):\n",
    "    for j in range(num_samples_per_image):\n",
    "        img = data_for_augmentation[i]\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        augmented_img = datagen.flow(img, batch_size=1, shuffle=False).next()\n",
    "        augmented_img = augmented_img.squeeze(axis=0)\n",
    "        augmented_images.append(augmented_img)\n",
    "        labels_aug.append(labels_for_augmentation[i])\n",
    "\n",
    "labels_aug=np.asarray(labels_aug)\n",
    "augmented_images=np.asarray(augmented_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train, augmented_images), axis=0)\n",
    "y_train = np.concatenate((y_train, labels_aug), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of observations : {len(y_train)}')\n",
    "\n",
    "print(f'appearances of 0 in labels : {np.count_nonzero(y_train == 0)}({round(100*np.count_nonzero(y_train == 0)/len(y_train),2)}% of total)')\n",
    "print(f'appearances of 1 in labels : {np.count_nonzero(y_train == 1)}({round(100*np.count_nonzero(y_train == 1)/len(y_train),2)}% of total)')\n",
    "print(f'appearances of 2 in labels : {np.count_nonzero(y_train == 2)}({round(100*np.count_nonzero(y_train == 2)/len(y_train),2)}% of total)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten data into 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = X_train.shape[0]\n",
    "image_size = X_train.shape[1] * X_train.shape[2] * X_train.shape[3]\n",
    "X_train = np.reshape(X_train, (num_samples, image_size))\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "num_samples = X_test.shape[0]\n",
    "image_size = X_test.shape[1] * X_test.shape[2] * X_test.shape[3]\n",
    "X_test = np.reshape(X_test, (num_samples, image_size))\n",
    "\n",
    "print(X_test.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfspeciale",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
